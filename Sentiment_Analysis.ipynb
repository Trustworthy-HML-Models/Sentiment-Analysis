{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####Problem 3: Sentiment Analysis"
      ],
      "metadata": {
        "id": "u-qD227VAN1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3DPlvXfftgZt"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary Libraries\n",
        "from tensorflow.keras.utils import get_file\n",
        "import tarfile\n",
        "from glob import glob\n",
        "import os,re,string\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dTrTNSxStp8V"
      },
      "outputs": [],
      "source": [
        "# Downloading Data from \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "data_directory = get_file('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', cache_subdir = \"datasets\",hash_algorithm = \"auto\", extract = True, archive_format = \"auto\")\n",
        "tar_file = tarfile.open(data_directory)\n",
        "# specifying the folder we need to extract\n",
        "tar_file.extractall('./data/') \n",
        "tar_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xrSV9j0Qt5Rg"
      },
      "outputs": [],
      "source": [
        "# Extracting data from downloaded files and loading the dataset for sentiment analysis\n",
        "#Specyfing the file path as per the problem statement\n",
        "file_path ='./data/aclImdb/'\n",
        "#specifying both the postive and negative files required for the analysis\n",
        "files = ['neg','pos']\n",
        "\n",
        "def load_dataset(file_path, folders):\n",
        "    texts,labels = [],[]\n",
        "    for i,label in enumerate(folders):\n",
        "        for fname in glob(os.path.join(file_path, label, '*.*')):\n",
        "            texts.append(open(fname, 'r').read())\n",
        "            labels.append(i)\n",
        "    \n",
        "    return texts, np.array(labels).astype(np.int64)\n",
        "\n",
        "x_train,y_train = load_dataset(f'{file_path}train',files)\n",
        "x_test,y_test = load_dataset(f'{file_path}test',files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VyK7M9Guwn0",
        "outputId": "eff79bfb-f5ae-40a0-aba4-4a9d76edacf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "25000\n"
          ]
        }
      ],
      "source": [
        "#Displaying the length of the training dataset \n",
        "print(len(x_train))\n",
        "print(len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N1glpHgdvs76"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Performing the data preprocessing steps:\n",
        "\n",
        "In order to facilitate the data interpretation raw texts obtained are preprocessed. First, elements such as punctuations, line breaks, numbers, and stop words like ‘a’, ‘the’, and ‘of’ are removed since they provide\n",
        "little information about the user’s impression towards a movie. Then, all the words are converted to lower cases and normalized to its true root.\n",
        "\n",
        "'''\n",
        "\n",
        "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "NO_SPACE = \"\"\n",
        "SPACE = \" \"\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    \n",
        "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
        "    \n",
        "    return reviews\n",
        "\n",
        "reviews_train_clean = preprocess_reviews(x_train)\n",
        "reviews_test_clean = preprocess_reviews(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUzfN3wNwAWp",
        "outputId": "684daa71-3058-49f8-8e37-a4c738eddffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "[\"after having seen and loved postal yes i actually loved postal i decided to try another uwe boll film and i picked out seed because i happened to stumble on it in a local dvd store and it's supposed to be one of his better films while the first  to  minutes of the film were very promising and seemed like the beginning of a not too mainstream psychological thriller it soon went downhill from there and eventually degraded into one of the most generic slasher films i've seen so far including a massive amount of plot holes unrealistic emotional responses and sub par acting it seems like boll tried his best to come up with a decent plot but after a while just gave up on it maybe he should stick to comedy the few good things about this film is that he does manage to create an overall creepy atmosphere that the special effects are better than i expected and the soundtrack does go well with the overall atmosphere but the unbalanced pacing of this film combined with the utter generic nature thereof makes he last half hour quite tedious to watch which ruined my experience altogether there are a very fairly well done shocking scenes but they seem to be there for the shock value alone and let's not forget the camera work that was pretty nauseating at times i hope uwe boll will one day learn what makes a good film because between a lot of horrible films he does seem to make a decent film every now and then seed just isn't one of those\"]\n"
          ]
        }
      ],
      "source": [
        "#veryfing the clean training dataset\n",
        "\n",
        "print(len(reviews_train_clean))\n",
        "print(reviews_train_clean[:1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC4zZwqlxrKW",
        "outputId": "189c682c-7c08-45c9-851e-01e0f3289229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for C=0.001: 0.87816\n",
            "Accuracy for C=0.005: 0.88424\n",
            "Accuracy for C=0.01: 0.88504\n",
            "Accuracy for C=0.05: 0.88536\n",
            "Accuracy for C=0.1: 0.8852\n",
            "Final Accuracy of training set: 0.94252\n",
            "Final Accuracy of test set: 0.90064\n"
          ]
        }
      ],
      "source": [
        "# Model - SVM n-gram Vectorization\n",
        "\n",
        "'''\n",
        "SVMs acknowledge the particular properties of text: (a) high dimensional feature spaces, (b) few irrelevant features (dense concept vector), and (c) sparse instance vectors.  SVMs consistently achieve good performance on text categorization tasks, outperforming existing methods substantially and signi\fcantly. With their ability to generalize well in high dimensional\n",
        "feature spaces, SVMs eliminate the need for feature selection, making the application of text categorization considerably easier. Hence SVM is used.\n",
        "\n",
        "Vectorization is the process of transforming the text data into numeric representations so that the data can be understandable by machine learning algorithms. \n",
        "Instead of just single-word tokens (1-gram/unigram) we can also include word pairs. n gram Vectorization along with SVM have been proven to work better for text classification problems apart from other pre trained models. So that model is considered for this problem.\n",
        "'''\n",
        "\n",
        "#Importing the required libraries for the model\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#By removing stop words, we remove the low-level information from our text in order to give more focus to the important information.\n",
        "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "#Spliting the data set into training and validation set\n",
        "X_train, X_val, y_training, y_val = train_test_split(X, y_train, test_size=0.5)\n",
        "\n",
        "# Regularisation parameter optimises the model. So trying to find the right value of c to get most accuracy\n",
        "for c in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
        "    \n",
        "    svm = LinearSVC(C=c)\n",
        "    svm.fit(X_train, y_training)\n",
        "    print (\"Accuracy for C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
        "        \n",
        "# Model with final selection of Regularization parameter   \n",
        "final = LinearSVC(C=0.01)\n",
        "final.fit(X_train, y_training)\n",
        "print (\"Final Accuracy of training set: %s\" \n",
        "       % accuracy_score(y_train, final.predict(X)))\n",
        "#Accuracy of test set\n",
        "final.fit(X, y_test)\n",
        "print (\"Final Accuracy of test set: %s\" \n",
        "       % accuracy_score(y_test, final.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "j7TauuddzmsY"
      },
      "outputs": [],
      "source": [
        "#saving the model in model folder \n",
        "\n",
        "import pickle\n",
        "file_name = \"models/Group46_NLP_model.h5\"\n",
        "pickle.dump(svm, open(file_name, 'wb'))\n",
        "pickle.dump(final, open(file_name, 'wb'))\n",
        "\n",
        "#loading the model\n",
        "svm = pickle.load(open(file_name,'rb'))\n",
        "final = pickle.load(open(file_name,'rb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6jCHH8AIBWh",
        "outputId": "e9f13e6d-db56-4c99-9fa8-a7384a08879a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy of test set: 0.90064\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print (\"Final Accuracy of test set: %s\" \n",
        "       % accuracy_score(y_test, final.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus SVM Linear SVC classifier with n-gram Vectorisation provides 94% accuracy for the IMDB large movie review dataset when 50% train test split is performed with C=0.01 and n - gram range between 1 to 3. Whereas, the models provides an accuracy of 90% for the test dataset. "
      ],
      "metadata": {
        "id": "oZLrdenNB7Ft"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_3_Question_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}